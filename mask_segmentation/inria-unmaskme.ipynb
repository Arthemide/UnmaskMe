{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## setup de l'environement","metadata":{"id":"iLpnCK-PL64w"}},{"cell_type":"code","source":"#from google.colab import drive\nimport os\nimport numpy as np\n#drive.mount('/content/drive',force_remount=True)\nos.chdir('/kaggle/input/celebamasked')\n","metadata":{"id":"FTS0-d8vykCJ","outputId":"47e3d546-48d0-46c2-f438-6a768fc818cc","execution":{"iopub.status.busy":"2021-11-11T01:51:40.591335Z","iopub.execute_input":"2021-11-11T01:51:40.591639Z","iopub.status.idle":"2021-11-11T01:51:40.600309Z","shell.execute_reply.started":"2021-11-11T01:51:40.591605Z","shell.execute_reply":"2021-11-11T01:51:40.599341Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle","metadata":{"execution":{"iopub.status.busy":"2021-11-11T02:02:04.919466Z","iopub.execute_input":"2021-11-11T02:02:04.919814Z","iopub.status.idle":"2021-11-11T02:02:05.682316Z","shell.execute_reply.started":"2021-11-11T02:02:04.919781Z","shell.execute_reply":"2021-11-11T02:02:05.680947Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def replace(dir):\n  for path in tqdm(os.listdir(dir)):\n    out_path_X = os.path.join(dir,path)\n    out_path_Y = os.path.join('_masked/',path.replace('_surgical', ''))\n    os.rename(out_path_X,out_path_Y)\n    \nreplace('test/x/')\nreplace('validation/x/')\n","metadata":{"id":"tMetgKbl_6T_","outputId":"0f7c53f7-89fe-4248-f48a-afaf66760742"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os \nprint(len(os.listdir('_masked/_masked')))\nprint(len(os.listdir('mask/mask')))\n","metadata":{"id":"yHTRai3ZYu-s","outputId":"d874b60c-b934-473d-ba5a-beae6a89363d","execution":{"iopub.status.busy":"2021-11-11T01:56:17.382991Z","iopub.execute_input":"2021-11-11T01:56:17.383494Z","iopub.status.idle":"2021-11-11T01:56:17.621832Z","shell.execute_reply.started":"2021-11-11T01:56:17.383458Z","shell.execute_reply":"2021-11-11T01:56:17.620584Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"os.path.exists('_masked/_masked/186753_surgical.jpg')","metadata":{"execution":{"iopub.status.busy":"2021-11-11T02:00:43.025584Z","iopub.execute_input":"2021-11-11T02:00:43.025845Z","iopub.status.idle":"2021-11-11T02:00:43.034415Z","shell.execute_reply.started":"2021-11-11T02:00:43.025816Z","shell.execute_reply":"2021-11-11T02:00:43.033148Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import shutil \n\ndef movedata(X_dir,Y_dir):\n  X_paths = os.listdir(X_dir)\n  length = 19699 #len(my_dir)\n  os.makedirs('/kaggle/working/train',exist_ok=True)\n  os.makedirs('/kaggle/working/train/x',exist_ok=True)\n  os.makedirs('/kaggle/working/train/y',exist_ok=True)\n  os.makedirs('/kaggle/working/validation',exist_ok=True)\n  os.makedirs('/kaggle/working/validation/x',exist_ok=True)\n  os.makedirs('/kaggle/working/validation/y',exist_ok=True)\n  os.makedirs('/kaggle/working/test',exist_ok=True)\n  os.makedirs('/kaggle/working/test/x',exist_ok=True)\n  os.makedirs('/kaggle/working/test/y',exist_ok=True)\n  \n  index = 0\n  acc = 0\n  l1 = length * 0.6\n  l2 = length * 0.8\n  for path in tqdm(X_paths):\n    file_ = path\n    if index < l1:\n      outdir = '/kaggle/working/train'  \n    elif index < l2 :\n      outdir = '/kaggle/working/validation'\n    else :\n      outdir = '/kaggle/working/test'\n    index += 1\n    origin_path_X = os.path.join(X_dir,file_)\n    origin_path_Y = os.path.join(Y_dir,file_.replace('_surgical',''))\n    \n    out_path_X = os.path.join(outdir,'x/',file_)\n    out_path_Y = os.path.join(outdir,'y/',file_)\n    #try :\n    shutil.move(origin_path_X,out_path_X)\n    shutil.move(origin_path_Y,out_path_Y) \n    #except :\n    #  acc +=1\n    \n  print(acc)\nmovedata('_masked/_masked/','mask/mask/')","metadata":{"id":"e1vDnj2glGDt","outputId":"8d1a3f55-c58b-4013-c0ad-c432b83f63bd","execution":{"iopub.status.busy":"2021-11-11T02:02:25.240477Z","iopub.execute_input":"2021-11-11T02:02:25.240809Z","iopub.status.idle":"2021-11-11T02:02:25.427195Z","shell.execute_reply.started":"2021-11-11T02:02:25.240774Z","shell.execute_reply":"2021-11-11T02:02:25.425945Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nimport torch.nn.functional as F\nfrom torchvision.transforms import ToTensor, Lambda, Compose\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\nfrom tqdm.auto import tqdm as tq\nfrom PIL import Image","metadata":{"id":"Uy3shNC7yqKf","execution":{"iopub.status.busy":"2021-11-11T01:53:57.123513Z","iopub.execute_input":"2021-11-11T01:53:57.123821Z","iopub.status.idle":"2021-11-11T01:53:59.057871Z","shell.execute_reply.started":"2021-11-11T01:53:57.123791Z","shell.execute_reply":"2021-11-11T01:53:59.056799Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Model - Unet","metadata":{"id":"BoemUMevMHce"}},{"cell_type":"code","source":"def train(n_epochs = 10):\n    # number of epochs to train the model\n    #n_epochs = 10\n    train_loss_list = []\n    valid_loss_list = []\n    dice_score_list = []\n    lr_rate_list = []\n    valid_loss_min = np.Inf # track change in validation loss\n    for epoch in range(1, n_epochs+1):\n\n        # keep track of training and validation loss\n        train_loss = 0.0\n        valid_loss = 0.0\n        dice_score = 0.0\n        model.train()\n        bar = tq(trainloader, postfix={\"train_loss\":0.0})\n        for data, target in bar:\n            # move tensors to GPU if CUDA is available\n            if train_on_gpu:\n                data, target = data.cuda(), target.cuda()\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # update training loss\n            train_loss += loss.item()*data.size(0)\n            bar.set_postfix(ordered_dict={\"train_loss\":loss.item()})\n        model.eval()\n        #del data, target\n        with torch.no_grad():\n            bar = tq(valloader, postfix={\"valid_loss\":0.0, \"dice_score\":0.0})\n            for data, target in bar:\n                # move tensors to GPU if CUDA is available\n                if train_on_gpu:\n                    data, target = data.cuda(), target.cuda()\n                # forward pass: compute predicted outputs by passing inputs to the model\n                output = model(data)\n                # calculate the batch loss\n                loss = criterion(output, target)\n                # update average validation loss \n                valid_loss += loss.item()*data.size(0)\n                dice_cof = dice_no_threshold(output.cpu(), target.cpu()).item()\n                dice_score +=  dice_cof * data.size(0)\n                bar.set_postfix(ordered_dict={\"valid_loss\":loss.item(), \"dice_score\":dice_cof})\n\n        # calculate average losses\n        train_loss = train_loss/len(trainloader.dataset)\n        valid_loss = valid_loss/len(valloader.dataset)\n        dice_score = dice_score/len(valloader.dataset)\n        train_loss_list.append(train_loss)\n        valid_loss_list.append(valid_loss)\n        dice_score_list.append(dice_score)\n        lr_rate_list.append([param_group['lr'] for param_group in optimizer.param_groups])\n\n        # print training/validation statistics \n        print('Epoch: {}  Training Loss: {:.6f}  Validation Loss: {:.6f} Dice Score: {:.6f}'.format(\n            epoch, train_loss, valid_loss, dice_score))\n\n        # save model if validation loss has decreased\n        if valid_loss <= valid_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n            valid_loss_min,\n            valid_loss))\n            torch.save(model.state_dict(), 'model_cifar.pt')\n            valid_loss_min = valid_loss\n    \n        scheduler.step(valid_loss)\n    return train_loss_list, valid_loss_list, dice_score_list, lr_rate_list","metadata":{"id":"1A5qeAoFyvw-","execution":{"iopub.status.busy":"2021-11-11T02:10:31.484164Z","iopub.execute_input":"2021-11-11T02:10:31.484437Z","iopub.status.idle":"2021-11-11T02:10:31.503557Z","shell.execute_reply.started":"2021-11-11T02:10:31.484408Z","shell.execute_reply":"2021-11-11T02:10:31.499966Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class double_conv(nn.Module):\n    \"\"\"(conv => BN => ReLU) * 2\"\"\"\n\n    def __init__(self, in_ch, out_ch):\n        super(double_conv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass inconv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(inconv, self).__init__()\n        self.conv = double_conv(in_ch, out_ch)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass down(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(down, self).__init__()\n        self.mpconv = nn.Sequential(nn.MaxPool2d(2), double_conv(in_ch, out_ch))\n\n    def forward(self, x):\n        x = self.mpconv(x)\n        return x\n\n\nclass up(nn.Module):\n    def __init__(self, in_ch, out_ch, bilinear=True):\n        super(up, self).__init__()\n\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode=\"nearest\", align_corners=True)\n        else:\n            self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)\n\n        self.conv = double_conv(in_ch, out_ch)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2))\n        \n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass outconv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(outconv, self).__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes):\n        super(UNet, self).__init__()\n        self.inc = inconv(n_channels, 64)\n        self.down1 = down(64, 128)\n        self.down2 = down(128, 256)\n        self.down3 = down(256, 512)\n        self.down4 = down(512, 512)\n        self.up1 = up(1024, 256, False)\n        self.up2 = up(512, 128, False)\n        self.up3 = up(256, 64, False)\n        self.up4 = up(128, 64, False)\n        self.outc = outconv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        x = self.outc(x)\n        return torch.sigmoid(x)","metadata":{"id":"HA1SVflgyvsj","execution":{"iopub.status.busy":"2021-11-11T02:10:32.290821Z","iopub.execute_input":"2021-11-11T02:10:32.291128Z","iopub.status.idle":"2021-11-11T02:10:32.314720Z","shell.execute_reply.started":"2021-11-11T02:10:32.291097Z","shell.execute_reply":"2021-11-11T02:10:32.313647Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## data loader","metadata":{"id":"d1f94B1FMO-6"}},{"cell_type":"code","source":"#@title Default title text\nimport torch\nfrom skimage.io import imread\nfrom torch.utils import data\nimport os\n\nclass dataset(data.Dataset):\n    def __init__(self, image_paths, target_paths, train='train'):   # initial logic happens like transform\n        self.train = train\n        self.src_image = image_paths\n        self.src_target = target_paths\n        self.image_paths = os.listdir(image_paths)\n        self.target_paths = os.listdir(target_paths)\n        self.transforms = transforms.Compose([\n                    transforms.Resize((128,128)),\n                    transforms.ToTensor(),\n                    ])\n        print('number of images:',self.__len__(), len(self.target_paths))\n        \n        \n        \n    def __getitem__(self, index):\n        #print(index)\n        if self.train == 'validation' :\n            index += len(self.image_paths) * 0.6\n        elif self.train == 'test' :\n            index += len(self.image_paths) * 0.8\n        #image = Image.open(os.path.join(self.src_image,next(self.image_paths).name)) \n        #mask = Image.open(os.path.join(self.src_target,next(self.target_paths).name)).convert('L')\n        image = Image.open(os.path.join(self.src_image,self.image_paths[index])) \n        mask = Image.open(os.path.join(self.src_target,self.image_paths[index].replace('_surgical',''))).convert('L')\n        \n        #image = np.array(image)\n        t_image = self.transforms(image)\n        t_mask = self.transforms(mask)\n        #t_image, t_mask = t_image.to(device), t_mask.to(device)\n        return t_image, t_mask\n\n    def __len__(self):  # return count of sample we have\n        if self.train == 'train':\n        #    return 100000\n            return int(len(self.target_paths) * 0.6) \n        return int(len(self.target_paths) * 0.2)\n               ","metadata":{"cellView":"code","id":"qbeW5CARyvmq","execution":{"iopub.status.busy":"2021-11-11T03:45:24.626314Z","iopub.execute_input":"2021-11-11T03:45:24.626737Z","iopub.status.idle":"2021-11-11T03:45:24.854935Z","shell.execute_reply.started":"2021-11-11T03:45:24.626684Z","shell.execute_reply":"2021-11-11T03:45:24.853580Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"import os \nprint(len(os.listdir('_masked')))\nprint(len(os.listdir('mask')))\n","metadata":{"id":"BDKJsbVXs7gY","outputId":"85b8d480-3444-4a7b-c464-cc635e91e117","execution":{"iopub.status.busy":"2021-11-11T02:10:57.355821Z","iopub.execute_input":"2021-11-11T02:10:57.356798Z","iopub.status.idle":"2021-11-11T02:10:57.367120Z","shell.execute_reply.started":"2021-11-11T02:10:57.356762Z","shell.execute_reply":"2021-11-11T02:10:57.365708Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"train_dataset = dataset('_masked/_masked/','mask/mask/','train')\nval_dataset   = dataset('_masked/_masked/','mask/mask/','validation')\nbatch_size = 8\nnum_workers = 2\ntrainloader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n)\nvalloader = DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n)","metadata":{"id":"qGmzKc7tyvey","outputId":"12780d41-2256-4c45-ded4-0677a08220d5","execution":{"iopub.status.busy":"2021-11-11T03:45:31.475244Z","iopub.execute_input":"2021-11-11T03:45:31.475504Z","iopub.status.idle":"2021-11-11T03:45:31.960184Z","shell.execute_reply.started":"2021-11-11T03:45:31.475475Z","shell.execute_reply":"2021-11-11T03:45:31.959174Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"x,y = train_dataset.__getitem__(0)\nprint(x.shape,y.shape)\nx,y = val_dataset.__getitem__(0)\nprint(x.shape,y.shape)\ny.mean()","metadata":{"id":"9Wu9ftswyvTs","outputId":"2fc88429-1085-4c98-cdb6-4c87792c19c7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training ","metadata":{"id":"9WVb1KfGMqO6"}},{"cell_type":"code","source":"import math\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n            \n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                buffered = self.buffer[int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    else:\n                        step_size = 1.0 / (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:            \n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n    \ndef f_score(pr, gt, beta=1, eps=1e-7, threshold=None, activation='sigmoid'):\n    \"\"\"\n    Args:\n        pr (torch.Tensor): A list of predicted elements\n        gt (torch.Tensor):  A list of elements that are to be predicted\n        eps (float): epsilon to avoid zero division\n        threshold: threshold for outputs binarization\n    Returns:\n        float: IoU (Jaccard) score\n    \"\"\"\n\n    if activation is None or activation == \"none\":\n        activation_fn = lambda x: x\n    elif activation == \"sigmoid\":\n        activation_fn = torch.nn.Sigmoid()\n    elif activation == \"softmax2d\":\n        activation_fn = torch.nn.Softmax2d()\n    else:\n        raise NotImplementedError(\n            \"Activation implemented for sigmoid and softmax2d\"\n        )\n\n    pr = activation_fn(pr)\n\n    if threshold is not None:\n        pr = (pr > threshold).float()\n\n\n    tp = torch.sum(gt * pr)\n    fp = torch.sum(pr) - tp\n    fn = torch.sum(gt) - tp\n\n    score = ((1 + beta ** 2) * tp + eps) \\\n            / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + eps)\n\n    return score\n\n\nclass DiceLoss(nn.Module):\n    __name__ = 'dice_loss'\n\n    def __init__(self, eps=1e-7, activation='sigmoid'):\n        super().__init__()\n        self.activation = activation\n        self.eps = eps\n\n    def forward(self, y_pr, y_gt):\n        return 1 - f_score(y_pr, y_gt, beta=1., \n                           eps=self.eps, threshold=None, \n                           activation=self.activation)\n\n\nclass BCEDiceLoss(DiceLoss):\n    __name__ = 'bce_dice_loss'\n\n    def __init__(self, eps=1e-7, activation='sigmoid', lambda_dice=1.0, lambda_bce=1.0):\n        super().__init__(eps, activation)\n        if activation == None:\n            self.bce = nn.BCELoss(reduction='mean')\n        else:\n            self.bce = nn.BCEWithLogitsLoss(reduction='mean')\n        self.lambda_dice=lambda_dice\n        self.lambda_bce=lambda_bce\n\n    def forward(self, y_pr, y_gt):\n        dice = super().forward(y_pr, y_gt)\n        bce = self.bce(y_pr, y_gt)\n        return (self.lambda_dice*dice) + (self.lambda_bce* bce)\n    \ndef dice_no_threshold(\n    outputs: torch.Tensor,\n    targets: torch.Tensor,\n    eps: float = 1e-7,\n    threshold: float = None,\n):\n    \"\"\"\n    Reference:\n    https://catalyst-team.github.io/catalyst/_modules/catalyst/dl/utils/criterion/dice.html\n    \"\"\"\n    if threshold is not None:\n        outputs = (outputs > threshold).float()\n\n    intersection = torch.sum(targets * outputs)\n    union = torch.sum(targets) + torch.sum(outputs)\n    dice = 2 * intersection / (union + eps)\n\n    return dice\n\ndef dice_no_threshold(\n    outputs: torch.Tensor,\n    targets: torch.Tensor,\n    eps: float = 1e-7,\n    threshold: float = None,\n):\n\n    if threshold is not None:\n        outputs = (outputs > threshold).float()\n\n    intersection = torch.sum(targets * outputs)\n    union = torch.sum(targets)\n    dice = intersection / (union)\n\n    return dice","metadata":{"id":"ievvC8rOz1io","execution":{"iopub.status.busy":"2021-11-11T02:11:50.488815Z","iopub.execute_input":"2021-11-11T02:11:50.489330Z","iopub.status.idle":"2021-11-11T02:11:50.581070Z","shell.execute_reply.started":"2021-11-11T02:11:50.489289Z","shell.execute_reply":"2021-11-11T02:11:50.579853Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"criterion = BCEDiceLoss(eps=1.0, activation=None)\noptimizer = RAdam(model.parameters(), lr = 0.005)\ncurrent_lr = [param_group['lr'] for param_group in optimizer.param_groups][0]\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=2, cooldown=2)\ntrain_on_gpu = torch.cuda.is_available()       \n","metadata":{"id":"Qv7h8webz1FZ","execution":{"iopub.status.busy":"2021-11-11T02:12:11.371978Z","iopub.execute_input":"2021-11-11T02:12:11.372261Z","iopub.status.idle":"2021-11-11T02:12:11.379375Z","shell.execute_reply.started":"2021-11-11T02:12:11.372232Z","shell.execute_reply":"2021-11-11T02:12:11.378286Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"LOAD = False\nPATH = './path'\nmodel = UNet(3,1).float()\nif LOAD :\n    model.load_state_dict(torch.load(PATH))\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device)","metadata":{"id":"8Acnfjqgytut","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice_coef_metric(pred, label):\n    intersection = 2.0 * (pred * label).sum()\n    union = pred.sum() + label.sum()\n    if pred.sum() == 0 and label.sum() == 0:\n        return 1.\n    return intersection / union\ndef dice_coef_loss(pred, label):\n    smooth = 1.0\n    intersection = 2.0 * (pred * label).sum() + smooth\n    union = pred.sum() + label.sum() + smooth\n    return 1 - (intersection / union)\ndef bce_dice_loss(pred, label):\n    dice_loss = dice_coef_loss(pred, label)\n    bce_loss = nn.BCELoss()(pred, label)\n    return dice_loss + bce_loss\ndef compute_iou(model, loader, threshold=0.3):\n    valloss = 0\n    with torch.no_grad():\n        for step, (data, target) in enumerate(loader):\n            data = data.to(device)\n            target = target.to(device)\n\n            outputs = model(data)\n            out_cut = np.copy(outputs.data.cpu().numpy())\n            out_cut[np.nonzero(out_cut < threshold)] = 0.0\n            out_cut[np.nonzero(out_cut >= threshold)] = 1.0\n\n            loss = dice_coef_metric(out_cut, target.data.cpu().numpy())\n            valloss += loss\n\n    return valloss / step","metadata":{"id":"2HFW5kN6fZzz","execution":{"iopub.status.busy":"2021-11-11T02:12:17.734087Z","iopub.execute_input":"2021-11-11T02:12:17.734886Z","iopub.status.idle":"2021-11-11T02:12:17.747519Z","shell.execute_reply.started":"2021-11-11T02:12:17.734851Z","shell.execute_reply":"2021-11-11T02:12:17.746404Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def train_model(train_loader, val_loader, loss_func, optimizer, scheduler, num_epochs):\n    loss_history = []\n    train_history = []\n    val_history = []\n    \n    for epoch in range(num_epochs):\n        model.train()\n        \n        losses = []\n        train_iou = []\n        \n        for i, (image, mask) in enumerate(tqdm(train_loader)):\n            image = image.to(device)\n            mask = mask.to(device)\n            outputs = model(image)\n            out_cut = np.copy(outputs.data.cpu().numpy())\n            out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n            out_cut[np.nonzero(out_cut >= 0.5)] = 1.0            \n            \n            train_dice = dice_coef_metric(out_cut, mask.data.cpu().numpy())\n            loss = loss_func(outputs, mask)\n            losses.append(loss.item())\n            train_iou.append(train_dice)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n                \n        #val_mean_iou = compute_iou(model, val_loader)\n        #scheduler.step(val_mean_iou)\n        loss_history.append(np.array(losses).mean())\n        train_history.append(np.array(train_iou).mean())\n        #val_history.append(val_mean_iou)\n        \n        print('Epoch : {}/{}'.format(epoch+1, num_epochs))\n        print('loss: {:.3f} - dice_coef: {:.3f} - val_dice_coef: '.format(np.array(losses).mean(),\n                                                                               np.array(train_iou).mean()\n                                                                               ))\n    return loss_history, train_history, val_history\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3)\n","metadata":{"id":"9mbc5LAKfHgl","execution":{"iopub.status.busy":"2021-11-11T02:12:19.440116Z","iopub.execute_input":"2021-11-11T02:12:19.440681Z","iopub.status.idle":"2021-11-11T02:12:19.453327Z","shell.execute_reply.started":"2021-11-11T02:12:19.440633Z","shell.execute_reply":"2021-11-11T02:12:19.452295Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train_loss_list, valid_loss_list, dice_score_list = train_model(trainloader,valloader,bce_dice_loss,optimizer,scheduler,1)\n#torch.save(model.state_dict(), './path')","metadata":{"id":"HfgP5QDtz03j","outputId":"0115c669-eb6a-4188-9899-6898fe35d781","execution":{"iopub.status.busy":"2021-11-11T03:46:18.598271Z","iopub.execute_input":"2021-11-11T03:46:18.598572Z","iopub.status.idle":"2021-11-11T04:14:10.849453Z","shell.execute_reply.started":"2021-11-11T03:46:18.598540Z","shell.execute_reply":"2021-11-11T04:14:10.848297Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/model.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ploting training ","metadata":{"id":"aeothuppM0pU"}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.plot(train_loss_list,  marker='o', label=\"Training Loss\")\nplt.plot(valid_loss_list,  marker='o', label=\"Validation Loss\")\nplt.ylabel('loss', fontsize=22)\nplt.legend()\nplt.show()","metadata":{"id":"kG_ATKrVZp8z","execution":{"iopub.status.busy":"2021-11-11T03:37:04.659476Z","iopub.execute_input":"2021-11-11T03:37:04.660266Z","iopub.status.idle":"2021-11-11T03:37:05.140006Z","shell.execute_reply.started":"2021-11-11T03:37:04.660234Z","shell.execute_reply":"2021-11-11T03:37:05.138797Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.plot(h.history['loss']);\nplt.plot(h.history['val_loss']);\nplt.title(\"SEG Model focal tversky Loss\");\nplt.ylabel(\"focal tversky loss\");\nplt.xlabel(\"Epochs\");\nplt.legend(['train', 'val']);\n\nplt.subplot(1,2,2)\nplt.plot(h.history['tversky']);\nplt.plot(h.history['val_tversky']);\nplt.title(\"SEG Model tversky score\");\nplt.ylabel(\"tversky Accuracy\");\nplt.xlabel(\"Epochs\");\nplt.legend(['train', 'val']);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fast_display(*img2dlist):\n    plt.figure(figsize=(16,8))\n    nbimg = len(img2dlist)\n    cols = min (9,nbimg)\n    rows = (nbimg // cols) +1\n    for ii, img2d in enumerate(img2dlist):\n        plt.subplot(rows,cols,1+ii)\n        plt.imshow(img2d)\n    plt.show()\n   ","metadata":{"id":"YaLyiQqu_vna","execution":{"iopub.status.busy":"2021-11-11T03:37:57.697394Z","iopub.execute_input":"2021-11-11T03:37:57.697760Z","iopub.status.idle":"2021-11-11T03:37:57.704584Z","shell.execute_reply.started":"2021-11-11T03:37:57.697715Z","shell.execute_reply":"2021-11-11T03:37:57.703150Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{"id":"sONgY_eXNPXz"}},{"cell_type":"code","source":" \nx,y = train_dataset.__getitem__(0)\nfor i, (data, target) in enumerate(trainloader):\n    if train_on_gpu:\n        data = data.cuda()\n    output = ((model(data))[0]).cpu().detach().numpy()\n    if i == 6:\n        break\nx = data.cpu().detach().numpy() \ny = target.cpu().detach().numpy() \n#x = np.moveaxis(x, 0, 2)\nprint(x.shape ,x.dtype)\nprint(y.shape ,y.dtype)\nprint(output.shape ,output.dtype)\nfast_display(x[0][0] ,y[0][0],output[0] )","metadata":{"id":"FKWSZ9eWz1aa","outputId":"2b341ab4-e959-47c7-fd83-c2ba6add2417","execution":{"iopub.status.busy":"2021-11-11T04:15:45.643226Z","iopub.execute_input":"2021-11-11T04:15:45.643530Z","iopub.status.idle":"2021-11-11T04:15:46.713154Z","shell.execute_reply.started":"2021-11-11T04:15:45.643495Z","shell.execute_reply":"2021-11-11T04:15:46.712162Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"fast_display(x[0] ,y[0] )","metadata":{"id":"vLRT3ms4z1Q3","outputId":"1a8908ae-7877-41ec-f1c0-29a10d17cf3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"IWBfRLh54X78"},"execution_count":null,"outputs":[]}]}